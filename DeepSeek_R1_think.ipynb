{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXgMRMZ7rQZ9+xjWnGSq2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junnan-cat/R1-THINK/blob/main/DeepSeek_R1_think.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The official [Readme](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#usage-recommendations) of DeepSeek R1 says:\n",
        "\n",
        "> Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.\n",
        "\n",
        "\n",
        "Here is the implementation (quite simple):\n",
        "\n",
        "I'll demonstrate how it works on Together AI's API. but it should also works on any other third-party cloud service or self-hosted server. (You may need to slightly change the code.)\n"
      ],
      "metadata": {
        "id": "TXQnQVYA_wAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = 'WRITE_YOUR_API_KEY_HERE'\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "  base_url=\"https://api.together.xyz/v1\",\n",
        ")\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "  model=\"deepseek-ai/DeepSeek-R1\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"WRITE_YOUR_PROMPTS_HERE\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"<think>\\n\"},\n",
        "  ],\n",
        "  stream=True,\n",
        "\n",
        ")\n",
        "\n",
        "print(\"<think>\\n\")\n",
        "for chunk in stream:\n",
        "  print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "fpR-sfMmCGym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the way, if you want to enforce the model NOT to think:"
      ],
      "metadata": {
        "id": "P7M79IAuCsXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = 'WRITE_YOUR_API_KEY_HERE'\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "  base_url=\"https://api.together.xyz/v1\",\n",
        ")\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "  model=\"deepseek-ai/DeepSeek-R1\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"WRITE_YOUR_PROMPTS_HERE\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"<think>\\n\\n</think>\"},\n",
        "  ],\n",
        "  stream=True,\n",
        "\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "  print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "Na4DM7njCziO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}